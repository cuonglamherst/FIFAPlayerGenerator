# -*- coding: utf-8 -*-
"""ITGAN_Lite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vrcrGGpVYXZ3X-zexN4SDxCYQXp0ODP
"""

# IT-GAN-Lite (Autoencoder + RealNVP + γ Density + WGAN-GP)
# Manh Cuong La · Binh Minh Nguyen


!pip install torch torchvision torchaudio pandas numpy scikit-learn seaborn matplotlib tqdm

import torch, torch.nn as nn, torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# Mount Google Drive

from google.colab import drive
drive.mount('/content/drive')

project_path = "/content/drive/MyDrive/ECE690_Final_Project/"
data_path = project_path + "data/"
output_path = project_path + "outputs/"

print("Project folder:", project_path)
print("Data folder:", data_path)
print("Output folder:", output_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)


# Gradient Penalty for WGAN-GP

def gradient_penalty(critic, real, fake):
    bs = real.size(0)
    eps = torch.rand(bs, 1).to(device)
    eps = eps.expand_as(real)

    interpolated = (eps * real + (1 - eps) * fake).requires_grad_(True)
    crit_interpolated = critic(interpolated)

    grad = torch.autograd.grad(
        outputs=crit_interpolated,
        inputs=interpolated,
        grad_outputs=torch.ones_like(crit_interpolated),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    grad_norm = grad.view(bs, -1).norm(2, dim=1)
    penalty = ((grad_norm - 1) ** 2).mean()
    return penalty

# Load FIFA Data + Normalize to [-1, 1]

# Load dataset with proper encoding
df = pd.read_csv(data_path + "player_stats.csv", encoding="latin1")

# Select relevant columns
cols = [
    'age','ball_control','dribbling','marking','slide_tackle','stand_tackle',
    'aggression','reactions','interceptions','vision','composure','crossing',
    'short_pass','long_pass','acceleration','sprint_speed','stamina','strength',
    'balance','shot_power','finishing','long_shots','curve','penalties'
]

# Keep only columns that exist in the dataset CSV
cols = [c for c in cols if c in df.columns]
df = df[cols]

df = df.dropna(axis=1, how='all')

df = df.apply(pd.to_numeric, errors='coerce')
df = df.fillna(df.mean(numeric_only=True))

df = df.loc[:, df.nunique() > 1]

print("Columns kept:", df.columns.tolist())
print("Shape after cleaning:", df.shape)
print("NaN count after cleaning:\n", df.isna().sum())

scaler = MinMaxScaler(feature_range=(-1, 1))
scaled = scaler.fit_transform(df)

# Replace any remaining NaNs or inf values after scaling
scaled = np.nan_to_num(scaled, nan=0.0, posinf=1.0, neginf=-1.0)

data = torch.tensor(scaled, dtype=torch.float32).to(device)
dataset = TensorDataset(data)
loader = DataLoader(dataset, batch_size=256, shuffle=True)

data_dim = data.shape[1]
latent_dim = 12

print("Final data dim:", data_dim)
print("Any NaNs in data:", torch.isnan(data).any().item())
print("Min:", data.min().item(), "Max:", data.max().item())

# Autoencoder (Encoder + Decoder)

class Encoder(nn.Module):
    def __init__(self, in_dim, latent_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 64), nn.ReLU(),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, latent_dim)
        )
    def forward(self, x):
        return self.net(x)

class Decoder(nn.Module):
    def __init__(self, latent_dim, out_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 32), nn.ReLU(),
            nn.Linear(32, 64), nn.ReLU(),
            nn.Linear(64, out_dim),
            nn.Tanh()   # keep output in [-1,1]
        )
    def forward(self, h):
        return self.net(h)

encoder = Encoder(data_dim, latent_dim).to(device)
decoder = Decoder(latent_dim, data_dim).to(device)

loss_recon = nn.MSELoss()
opt_ae = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=2e-4)

print("Autoencoder initialized.")
print(encoder)
print(decoder)

# RealNVP Flow (Invertible Generator)

class CouplingLayer(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # Networks for scale and translation
        self.scale = nn.Sequential(
            nn.Linear(dim//2, 64), nn.ReLU(),
            nn.Linear(64, dim//2), nn.Tanh()
        )
        self.trans = nn.Sequential(
            nn.Linear(dim//2, 64), nn.ReLU(),
            nn.Linear(64, dim//2)
        )

    def forward(self, x, reverse=False):
        # Split into two halves
        x1, x2 = x.chunk(2, dim=1)

        # Compute scale and shift
        s = self.scale(x1)
        t = self.trans(x1)

        if not reverse:
            # Forward affine transform
            y2 = x2 * torch.exp(s) + t
            logdet = s.sum(1)
        else:
            # Reverse transform
            y2 = (x2 - t) * torch.exp(-s)
            logdet = -s.sum(1)

        # Concatenate halves
        y = torch.cat([x1, y2], dim=1)
        return y, logdet


class RealNVPFlow(nn.Module):
    def __init__(self, dim, n_layers=4):
        super().__init__()
        # Stack coupling layers
        self.layers = nn.ModuleList([CouplingLayer(dim) for _ in range(n_layers)])

    def forward(self, z):
        # Forward latent transform
        h = z
        logdet_sum = 0
        for layer in self.layers:
            h, logdet = layer(h, reverse=False)
            logdet_sum += logdet
        return h, logdet_sum

    def inverse(self, h):
        # Reverse latent transform
        z = h
        for layer in reversed(self.layers):
            z, _ = layer(z, reverse=True)
        return z


# Initialize flow
flow = RealNVPFlow(latent_dim).to(device)
opt_flow = optim.Adam(flow.parameters(), lr=2e-4)

print("RealNVP Flow initialized.")
flow

# WGAN-GP Critic

class Critic(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # Simple MLP critic
        self.net = nn.Sequential(
            nn.Linear(dim, 64), nn.LeakyReLU(0.2),
            nn.Linear(64, 32), nn.LeakyReLU(0.2),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.net(x)

critic = Critic(data_dim).to(device)
opt_critic = optim.Adam(critic.parameters(), lr=1e-4)

print("WGAN-GP Critic initialized.")
critic

print("Any NaNs in data:", torch.isnan(data).any().item())
print("Min value:", data.min().item())
print("Max value:", data.max().item())

# Training Loop

gamma = 0.1          # weight for density regularizer
critic_steps = 5     # critic is updated multiple times per batch
epochs = 300

for epoch in range(epochs):
    for batch in loader:
        real_x = batch[0]

        # Autoencoder update: learn latent representation h and reconstruct x
        opt_ae.zero_grad()
        h = encoder(real_x)        # encode real data
        recon_x = decoder(h)       # decode back to data space
        ae_loss = loss_recon(recon_x, real_x)   # reconstruction error
        ae_loss.backward()
        opt_ae.step()

        # Critic update: estimate Wasserstein distance (real vs fake)
        # Multiple critic updates improve stability
        for _ in range(critic_steps):
            opt_critic.zero_grad()

            real_score = critic(real_x)   # critic output on real data

            # Generate fake samples through flow and decoder
            z = torch.randn(real_x.size(0), latent_dim).to(device)
            h_fake, _ = flow(z)           # generate latent code
            x_fake = decoder(h_fake).detach()  # detach: only critic learns here

            fake_score = critic(x_fake)   # critic output on generated data

            # Gradient penalty enforces Lipschitz constraint
            gp = gradient_penalty(critic, real_x, x_fake)

            # WGAN-GP loss: critic tries to maximize real - fake
            loss_c = -(real_score.mean() - fake_score.mean()) + 10 * gp
            loss_c.backward()
            opt_critic.step()

        # Generator update: flow learns to fool critic and match density
        opt_flow.zero_grad()

        z = torch.randn(real_x.size(0), latent_dim).to(device)
        h_fake, logdet = flow(z)       # generate latent sample + log-det Jacobian
        x_fake = decoder(h_fake)       # decode latent to data space

        adv_loss = -critic(x_fake).mean()   # generator tries to increase critic score
        density_loss = -gamma * logdet.mean()   # density regularizer from IT-GAN
        g_loss = adv_loss + density_loss

        g_loss.backward()
        opt_flow.step()

    # Print losses to monitor training
    print(f"[Epoch {epoch}] AE={ae_loss.item():.4f} | C={loss_c.item():.4f} | G={g_loss.item():.4f}")

# Generate Synthetic Players

gen_samples = 2000   # number of fake players to generate

# Sample noise
z = torch.randn(gen_samples, latent_dim).to(device)

# Generate latent h and decode to player stats
with torch.no_grad():
    h_fake, _ = flow(z)
    x_fake = decoder(h_fake).cpu().numpy()

# Invert scaling to original FIFA stat range
fake_rescaled = scaler.inverse_transform(x_fake)
real_rescaled = scaler.inverse_transform(data.cpu().numpy())

# Convert to DataFrame
fake_df = pd.DataFrame(fake_rescaled, columns=df.columns)
real_df = pd.DataFrame(real_rescaled, columns=df.columns)

print("Generated synthetic players:", fake_df.shape)
display(fake_df.head())

# Save results
fake_df.to_csv(output_path + "generated_players.csv", index=False)
print("Generated players saved to Drive.")

# Evaluation

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import wasserstein_distance
from sklearn.decomposition import PCA
import numpy as np


# KDE Distribution Comparison
features = df.columns.tolist()
rows = 3
cols = 8  # wide layout for slide readability

plt.figure(figsize=(22, 10))

for i, col in enumerate(features):
    plt.subplot(rows, cols, i + 1)
    sns.kdeplot(real_df[col], label="Real", fill=True, alpha=0.3)
    sns.kdeplot(fake_df[col], label="Fake", fill=True, alpha=0.3)
    plt.title(col, fontsize=9)
    plt.xticks(fontsize=7)
    plt.yticks(fontsize=7)

plt.tight_layout()
plt.savefig(output_path + "kde_all_features.png", dpi=300)
plt.show()

# Summary Statistics Table
compare_stats = pd.DataFrame({
    "Real Mean": real_df.mean(),
    "Fake Mean": fake_df.mean(),
    "Real Std": real_df.std(),
    "Fake Std": fake_df.std()
}).round(2)

display(compare_stats)
compare_stats.to_csv(output_path + "summary_stats.csv", index=False)


# Wasserstein Distances
distances = {
    col: wasserstein_distance(real_df[col], fake_df[col])
    for col in df.columns
}

wd_df = pd.DataFrame(
    distances.items(),
    columns=["Feature", "WassersteinDistance"]
).sort_values("WassersteinDistance")

display(wd_df)
wd_df.to_csv(output_path + "wasserstein_distances.csv", index=False)


# Horizontal Wasserstein Bar Plot
plt.figure(figsize=(18, 6))
sns.barplot(
    data=wd_df,
    y="Feature",
    x="WassersteinDistance",
    palette="viridis",
    orient="h"
)

plt.title("Feature Similarity (Lower = Better)")
plt.xlabel("Wasserstein Distance")
plt.ylabel("Feature")
plt.tight_layout()
plt.savefig(output_path + "wasserstein_barplot.png", dpi=300)
plt.show()



# Rotated Correlation Heatmaps
import matplotlib.pyplot as plt
import seaborn as sns

corr_real = real_df.corr()
corr_fake = fake_df.corr()

fig, axes = plt.subplots(
    nrows=2, ncols=2,
    figsize=(10, 18),  # tall figure
    gridspec_kw={'width_ratios': [1, 0.05]}  # right column = thin colorbars
)


# REAL DATA HEATMAP
ax_real = axes[0,0]
cbar_real = axes[0,1]  # dedicated small axis for colorbar

sns.heatmap(
    corr_real,
    cmap="coolwarm",
    center=0,
    cbar=True,
    cbar_ax=cbar_real,   # ensures identical height
    ax=ax_real
)

ax_real.set_title("Real Data Correlation")


# FAKE DATA HEATMAP
ax_fake = axes[1,0]
cbar_fake = axes[1,1]  # dedicated small axis for colorbar

sns.heatmap(
    corr_fake,
    cmap="coolwarm",
    center=0,
    cbar=True,
    cbar_ax=cbar_fake,   # ensures identical height
    ax=ax_fake
)

ax_fake.set_title("Fake Data Correlation")



# Final layout
plt.tight_layout()
plt.savefig(output_path + "correlation_heatmaps.png", dpi=300)
plt.show()


# PCA Visualization
pca = PCA(n_components=2)
pca_real = pca.fit_transform(real_df)
pca_fake = pca.transform(fake_df)

plt.figure(figsize=(10, 6))
plt.scatter(pca_real[:, 0], pca_real[:, 1], alpha=0.25, label="Real")
plt.scatter(pca_fake[:, 0], pca_fake[:, 1], alpha=0.25, label="Fake")
plt.legend()
plt.title("PCA: Real vs Fake Player Distribution")
plt.tight_layout()
plt.savefig(output_path + "pca_real_vs_fake.png", dpi=300)
plt.show()